# LLM-Fencing-Jailbreaking
This project includes a collection of 6 jailbreaking and/or red-teaming datasets from the following sources:

BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset
https://arxiv.org/abs/2307.04657

Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study
https://arxiv.org/abs/2305.13860

Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models
https://arxiv.org/abs/2307.08487

REALTOXICITYPROMPTS: Evaluating Neural Toxic Degeneration in Language Models
https://arxiv.org/abs/2009.11462

Safety Assessment of Chinese Large Language Models
https://arxiv.org/abs/2304.10436

Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback
https://arxiv.org/abs/2204.05862

