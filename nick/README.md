# Red Teaming for Large Language Models (LLM's) - Resources

This repository contains a curated list of resources, papers, datasets, and frameworks related to evaluating red teaming for Large Language Models (LLM's).

## Table of Contents
- [Helpful and Harmless Assistant with RLHF](#helpful-and-harmless-assistant-with-rlhf)
- [PromptBench: Robustness Evaluation Framework](#promptbench-robustness-evaluation-framework)
- [Red Teaming Language Models from Scratch](#red-teaming-language-models-from-scratch)
- [Jailbreaker: Automated Jailbreak](#jailbreaker-automated-jailbreak)
- [BeaverTails: Human-Preference Dataset](#beavertails-human-preference-dataset)
- [Visual Adversarial Examples](#visual-adversarial-examples)
- [Red Teaming Language Models with Language Models](#red-teaming-language-models-with-language-models)
- [Red Teaming Language Models to Reduce Harms](#red-teaming-language-models-to-reduce-harms)

### Helpful and Harmless Assistant with RLHF
- [GitHub Repository](https://github.com/anthropics/hh-rlhf)
- Paper: ["Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"](https://arxiv.org/abs/2204.05862)
  - Referenced for methods in [PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts](https://arxiv.org/abs/2306.04528)

### PromptBench: Robustness Evaluation Framework
- [arXiv Paper](https://arxiv.org/abs/2306.04528)
- [HuggingFace Demo](https://huggingface.co/spaces/March07/PromptBench)
- [GitHub Repository](https://github.com/microsoft/promptbench)
- [TextAttack: Adversarial Attacks Framework](https://github.com/QData/TextAttack)

### Red Teaming Language Models from Scratch
- [arXiv Paper](https://arxiv.org/abs/2306.09442)
- [GitHub Repository](https://github.com/thestephencasper/explore_establish_exploit_llms)
- [CommonClaim Dataset](https://github.com/Algorithmic-Alignment-Lab/CommonClaim)

### Jailbreaker: Automated Jailbreak
- [arXiv Paper](https://arxiv.org/abs/2307.08715)
- [Accompanying Site](https://sites.google.com/view/ndss-masterkey/masterkey) (Incomplete)

### BeaverTails: Human-Preference Dataset
- [arXiv Paper](https://arxiv.org/abs/2307.04657)
- [Accompanying Site](https://sites.google.com/view/pku-beavertails)
- [Dataset](https://github.com/PKU-Alignment/beavertails)

### Visual Adversarial Examples
- [arXiv Paper](https://arxiv.org/abs/2306.13213)
- [Detoxify GitHub Repository](https://github.com/unitaryai/detoxify)
- [Real Toxicity Prompts](https://allenai.org/data/real-toxicity-prompts) (Visual Focus)

### Red Teaming Language Models with Language Models
- [arXiv Paper](https://arxiv.org/abs/2202.03286) (No datasets; Exploration of Red Teaming LMs using another LM)

### Red Teaming Language Models to Reduce Harms
- [arXiv Paper](https://arxiv.org/abs/2209.07858)
- [Same GitHub as RLHF](https://github.com/anthropics/hh-rlhf)

---

*Note: Always check the appropriate licensing and usage information for each resource.*
